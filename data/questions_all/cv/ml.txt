00.小点#00_小点.md
00.李航统计学习方法资源汇总（有时间一定要系统的学一遍）#00_李航统计学习方法资源汇总（有时间一定要系统的学一遍）.md
01.LR和SVM的比较#01_LR和SVM的比较.md
101.信息熵、交叉熵、相对熵#101_信息熵、交叉熵、相对熵.md
102.PR曲线F1评分ROC曲线AUC#102_PR曲线F1评分ROC曲线AUC.md
103.匈牙利、KM匹配#103_匈牙利、KM匹配.md
104.MOSSE、Kalman、KCF（？？？）#104_MOSSE、Kalman、KCF（？？？）.md
105.最大似然估计+最大期望算法（？？？）#105_最大似然估计+最大期望算法（？？？）.md
106.多重共线性、岭回归、Lasso#106_多重共线性、岭回归、Lasso.md
107.SVM常见面试问题？？？#107_SVM常见面试问题？？？.md
108.有监督、无监督、自监督、半监督、弱监督的区别#108_有监督、无监督、自监督、半监督、弱监督的区别.md
109.极大似然估计、最大后验估计、贝叶斯估计#109_极大似然估计、最大后验估计、贝叶斯估计.md
110.各种距离计算公式（余弦、欧式、马氏）#110_各种距离计算公式（余弦、欧式、马氏）.md
111.RANSAC介绍#111_RANSAC介绍.md
112.SVM和LR的详细推导？？？#112_SVM和LR的详细推导？？？.md
113.NLP相关知识#113_NLP相关知识.md
11.三种主要集成学习思想简介#11_三种主要集成学习思想简介.md
12.Adaboost_GBDT_XGBoost算法原理#12_Adaboost_GBDT_XGBoost算法原理.md
13.GBDT详解#13_GBDT详解.md
17.bagging算法思想及与DNN中的dropout思想的一致性#17_bagging算法思想及与DNN中的dropout思想的一致性.md
19.如何从偏差和方差的角度解释bagging和boosting的原理#19_如何从偏差和方差的角度解释bagging和boosting的原理.md
20.随机森林思想#20_随机森林思想.md
26.k-means算法原理#26_k-means算法原理.md
28.k-means和KNN的区别与联系#28_k-means和KNN的区别与联系.md
29.k-means和GMM的区别与联系#29_k-means和GMM的区别与联系.md
30.剪枝#30_剪枝.md
32.决策树缺失值处理方法#32_决策树缺失值处理方法.md
33.（已丢弃，直接看34）决策树的原理以及决策树如何生成#33_（已丢弃，直接看34）决策树的原理以及决策树如何生成.md
34.ID3_C4.5_CART算法总结与对比#34_ID3_C4.5_CART算法总结与对比.md
44.PCA算法原理#44_PCA算法原理.md
45.LDA算法原理#45_LDA算法原理.md
46.PCA与LDA比较#46_PCA与LDA比较.md
49.判别式模型和生成式模型#49_判别式模型和生成式模型.md
57.机器学习中常用的损失函数一览#57_机器学习中常用的损失函数一览.md
58.线性回归损失函数为什么要用平方形式#58_线性回归损失函数为什么要用平方形式.md
59.逻辑回归与线性回归之间的异同#59_逻辑回归与线性回归之间的异同.md
重点学习#重点学习.md
60.LR中的损失函数为什么不能写成二值的交叉熵的那种形式呢？两者有什么区别？#60.LR中的损失函数为什么不能写成二值的交叉熵的那种形式呢？两者有什么区别？.md
